{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVUts8GW3G_G",
        "outputId": "a3614594-cda6-43f6-90ab-93f8531fae3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairlearn\n",
            "  Downloading fairlearn-0.12.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.3->fairlearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.3->fairlearn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.3->fairlearn) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.1->fairlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.1->fairlearn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.17.0)\n",
            "Downloading fairlearn-0.12.0-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fairlearn\n",
            "Successfully installed fairlearn-0.12.0\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install fairlearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from fairlearn.metrics import MetricFrame\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load Data\n",
        "train_df = pd.read_csv('/content/drive/My Drive/BTTKaggleCompetition/bttai-ajl-2025/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/My Drive/BTTKaggleCompetition/bttai-ajl-2025/test.csv')\n",
        "\n",
        "# Preprocess file paths\n",
        "train_df['file_path'] = '/content/drive/My Drive/BTTKaggleCompetition/bttai-ajl-2025/train/train/' + train_df['label'] + '/' + train_df['md5hash'] + '.jpg'\n",
        "test_df['file_path'] = '/content/drive/My Drive/BTTKaggleCompetition/bttai-ajl-2025/test/test/' + test_df['md5hash'] + '.jpg'\n",
        "\n",
        "# Create validation split\n",
        "train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['label'])\n",
        "\n",
        "# Create label encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_data['encoded_label'] = label_encoder.fit_transform(train_data['label'])\n",
        "val_data['encoded_label'] = label_encoder.transform(val_data['label'])\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mCEeJEfUqGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf204a9a-c7d5-4203-9b76-1d7b1fa78bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Found 1013 validated image filenames.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 1275 invalid image filename(s) in x_col=\"file_path\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 253 validated image filenames.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 319 invalid image filename(s) in x_col=\"file_path\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Create the ResNet50 Model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',  # Changed from categorical_crossentropy\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Compute Class Weights\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_data['encoded_label']),\n",
        "    y=train_data['encoded_label']\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Create generators\n",
        "batch_size = 32\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "    train_data,\n",
        "    x_col='file_path',\n",
        "    y_col='encoded_label',  # Using encoded labels\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='raw'  # Changed to raw for sparse categorical crossentropy\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_dataframe(\n",
        "    val_data,\n",
        "    x_col='file_path',\n",
        "    y_col='encoded_label',  # Using encoded labels\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    class_mode='raw'  # Changed to raw for sparse categorical crossentropy\n",
        ")\n",
        "\n",
        "# Training callbacks\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs= 32,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXIlMrHHGSNs",
        "outputId": "122852f3-5862-43c5-c02c-8d0e7470cf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 10s/step - accuracy: 0.1747 - loss: 4.3509 - val_accuracy: 0.4704 - val_loss: 1.6521 - learning_rate: 0.0010\n",
            "Epoch 2/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 8s/step - accuracy: 0.4241 - loss: 1.9835 - val_accuracy: 0.5494 - val_loss: 1.4837 - learning_rate: 0.0010\n",
            "Epoch 3/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 8s/step - accuracy: 0.4993 - loss: 1.6606 - val_accuracy: 0.5692 - val_loss: 1.4143 - learning_rate: 0.0010\n",
            "Epoch 4/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 9s/step - accuracy: 0.5735 - loss: 1.4181 - val_accuracy: 0.5613 - val_loss: 1.3464 - learning_rate: 0.0010\n",
            "Epoch 5/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 9s/step - accuracy: 0.5969 - loss: 1.3195 - val_accuracy: 0.5494 - val_loss: 1.3608 - learning_rate: 0.0010\n",
            "Epoch 6/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 8s/step - accuracy: 0.6185 - loss: 1.2366 - val_accuracy: 0.5652 - val_loss: 1.3467 - learning_rate: 0.0010\n",
            "Epoch 7/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 8s/step - accuracy: 0.6935 - loss: 1.0608 - val_accuracy: 0.6047 - val_loss: 1.3082 - learning_rate: 0.0010\n",
            "Epoch 8/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 8s/step - accuracy: 0.6706 - loss: 1.0405 - val_accuracy: 0.6126 - val_loss: 1.2993 - learning_rate: 0.0010\n",
            "Epoch 9/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 8s/step - accuracy: 0.7016 - loss: 0.9794 - val_accuracy: 0.6245 - val_loss: 1.1550 - learning_rate: 0.0010\n",
            "Epoch 10/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 8s/step - accuracy: 0.7146 - loss: 0.8908 - val_accuracy: 0.6561 - val_loss: 1.1946 - learning_rate: 0.0010\n",
            "Epoch 11/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 9s/step - accuracy: 0.7316 - loss: 0.8723 - val_accuracy: 0.6047 - val_loss: 1.3081 - learning_rate: 0.0010\n",
            "Epoch 12/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 8s/step - accuracy: 0.7265 - loss: 0.8533 - val_accuracy: 0.6403 - val_loss: 1.2765 - learning_rate: 0.0010\n",
            "Epoch 13/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 9s/step - accuracy: 0.7538 - loss: 0.7768 - val_accuracy: 0.6522 - val_loss: 1.2393 - learning_rate: 0.0010\n",
            "Epoch 14/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 9s/step - accuracy: 0.7463 - loss: 0.7404 - val_accuracy: 0.6403 - val_loss: 1.2177 - learning_rate: 0.0010\n",
            "Epoch 15/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 8s/step - accuracy: 0.8148 - loss: 0.5763 - val_accuracy: 0.6759 - val_loss: 1.2322 - learning_rate: 2.0000e-04\n",
            "Epoch 16/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 9s/step - accuracy: 0.8668 - loss: 0.4722 - val_accuracy: 0.6798 - val_loss: 1.1630 - learning_rate: 2.0000e-04\n",
            "Epoch 17/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 9s/step - accuracy: 0.8810 - loss: 0.4342 - val_accuracy: 0.6482 - val_loss: 1.2313 - learning_rate: 2.0000e-04\n",
            "Epoch 18/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 9s/step - accuracy: 0.8620 - loss: 0.4675 - val_accuracy: 0.6640 - val_loss: 1.1317 - learning_rate: 2.0000e-04\n",
            "Epoch 19/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 8s/step - accuracy: 0.8648 - loss: 0.4478 - val_accuracy: 0.6917 - val_loss: 1.1651 - learning_rate: 2.0000e-04\n",
            "Epoch 20/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 8s/step - accuracy: 0.8874 - loss: 0.3394 - val_accuracy: 0.6838 - val_loss: 1.1750 - learning_rate: 2.0000e-04\n",
            "Epoch 21/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 8s/step - accuracy: 0.8919 - loss: 0.4069 - val_accuracy: 0.6522 - val_loss: 1.2279 - learning_rate: 2.0000e-04\n",
            "Epoch 22/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 9s/step - accuracy: 0.8814 - loss: 0.3918 - val_accuracy: 0.6640 - val_loss: 1.2554 - learning_rate: 2.0000e-04\n",
            "Epoch 23/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 8s/step - accuracy: 0.8650 - loss: 0.4103 - val_accuracy: 0.7194 - val_loss: 1.2028 - learning_rate: 2.0000e-04\n",
            "Epoch 24/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 8s/step - accuracy: 0.8779 - loss: 0.3649 - val_accuracy: 0.6640 - val_loss: 1.1815 - learning_rate: 4.0000e-05\n",
            "Epoch 25/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 9s/step - accuracy: 0.8908 - loss: 0.3541 - val_accuracy: 0.7154 - val_loss: 1.1473 - learning_rate: 4.0000e-05\n",
            "Epoch 26/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 8s/step - accuracy: 0.9022 - loss: 0.3586 - val_accuracy: 0.7075 - val_loss: 1.1379 - learning_rate: 4.0000e-05\n",
            "Epoch 27/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 8s/step - accuracy: 0.8937 - loss: 0.3303 - val_accuracy: 0.6759 - val_loss: 1.1835 - learning_rate: 4.0000e-05\n",
            "Epoch 28/32\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 9s/step - accuracy: 0.8989 - loss: 0.3494 - val_accuracy: 0.6917 - val_loss: 1.1869 - learning_rate: 4.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yplLP16PP3v0",
        "outputId": "88891796-ece6-4779-c7bb-7f074fdb0ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step \n",
            "Weighted F1-Score: 0.0456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        }
      ],
      "source": [
        "# Ensure labels match validation data\n",
        "val_labels = val_data['encoded_label'].values\n",
        "\n",
        "# Predict using the full validation set\n",
        "steps = int(np.ceil(len(val_data) / val_generator.batch_size))  # Ensure all data is processed\n",
        "val_preds = model.predict(val_generator, steps=steps, verbose=1)\n",
        "val_preds = np.argmax(val_preds, axis=1)\n",
        "\n",
        "# Ensure val_labels length matches val_preds\n",
        "val_labels = val_labels[:len(val_preds)]\n",
        "\n",
        "# Compute weighted F1-score\n",
        "weighted_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "print(f'Weighted F1-Score: {weighted_f1:.4f}')\n",
        "\n",
        "\n",
        "# Fairness Evaluation (if skin_tone column exists)\n",
        "if 'skin_tone' in val_data.columns:\n",
        "    skin_tone_labels = val_data['skin_tone'].values\n",
        "    metric_frame = MetricFrame(\n",
        "        metrics=f1_score,\n",
        "        y_true=val_labels,\n",
        "        y_pred=val_preds,\n",
        "        sensitive_features=skin_tone_labels\n",
        "    )\n",
        "    print(\"\\nF1-Score by Skin Tone:\")\n",
        "    print(metric_frame.by_group)\n",
        "\n",
        "# Grad-CAM visualization function\n",
        "def get_grad_cam(model, img_array, layer_name='conv5_block3_out'):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs],\n",
        "        [model.get_layer(layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        loss = predictions[:, np.argmax(predictions[0])]\n",
        "\n",
        "    grads = tape.gradient(loss, conv_outputs)[0]\n",
        "    pooled_grads = np.mean(grads, axis=(0, 1, 2))\n",
        "    conv_outputs = conv_outputs[0]\n",
        "\n",
        "    for i in range(pooled_grads.shape[-1]):\n",
        "        conv_outputs[:, :, i] *= pooled_grads[i]\n",
        "\n",
        "    heatmap = np.mean(conv_outputs, axis=-1)\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap /= np.max(heatmap)\n",
        "    return heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "kdnfY0Jvmb7L",
        "outputId": "35776847-6330-4b88-af72-55ee8c5fe3b3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'batch_size' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6581fcd2b438>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'file_path'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m  \u001b[0;31m# No labels for the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
          ]
        }
      ],
      "source": [
        "# Ensure datagen is defined before using it for test data\n",
        "test_generator = datagen.flow_from_dataframe(\n",
        "    test_df,\n",
        "    x_col='file_path',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    class_mode=None  # No labels for the test set\n",
        ")\n",
        "\n",
        "# Generate predictions\n",
        "steps = int(np.ceil(len(test_df) / test_generator.batch_size))  # Ensure all samples are processed\n",
        "test_preds = model.predict(test_generator, steps=steps, verbose=1)\n",
        "test_labels = np.argmax(test_preds, axis=1)\n",
        "test_df['label'] = label_encoder.inverse_transform(test_labels)\n",
        "\n",
        "# Save Submission File\n",
        "test_df[['md5hash', 'label']].to_csv('submission.csv', index=False)\n",
        "print(\"submission.csv saved successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}